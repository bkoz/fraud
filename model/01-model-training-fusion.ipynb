{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Demo\n",
    "\n",
    "Based on: https://www.kaggle.com/code/zwhjorth/dnn-svm-and-dt-for-fraud-detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all requirements needed to train this model and track it in MLFlow.\n",
    "\n",
    "!pip install pip numpy pandas tensorflow matplotlib seaborn tf2onnx onnxruntime scikit-learn boto3 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies we need to run the code.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import tf2onnx\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV data which we will use to train the model.\n",
    "# It contains the following fields:\n",
    "#   distancefromhome - The distance from home where the transaction happened.\n",
    "#   distancefromlast_transaction - The distance from last transaction happened.\n",
    "#   ratiotomedianpurchaseprice - Ratio of purchased price compared to median purchase price.\n",
    "#   repeat_retailer - If it's from a retailer that already has been purchased from before.\n",
    "#   used_chip - If the (credit card) chip was used.\n",
    "#   usedpinnumber - If the PIN number was used.\n",
    "#   online_order - If it was an online order.\n",
    "#   fraud - If the transaction is fraudulent.\n",
    "\n",
    "Data = pd.read_csv('../data/card_transdata.csv')\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input (X) and output (Y) data. \n",
    "# The only output data we have is if it's fraudulent or not, and all other fields go as inputs to the model.\n",
    "\n",
    "X = Data.drop(columns = ['fraud'])\n",
    "y = Data['fraud']\n",
    "\n",
    "# Split the data into training and testing sets so we have something to test the trained model with.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, stratify = y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size = 0.2, stratify = y_train)\n",
    "\n",
    "# Scale the data to remove mean and have unit variance. This means that the data will be between -1 and 1, which makes it a lot easier for the model to learn than random potentially large values.\n",
    "# It is important to only fit the scaler to the training data, otherwise you are leaking information about the global distribution of variables (which is influenced by the test set) into the training set.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Since the dataset is unbalanced (it has many more non-fraud transactions than fraudulent ones), we set a class weight to weight the few fraudulent transactions higher than the many non-fraud transactions.\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes = np.unique(y_train),y = y_train)\n",
    "class_weights = {i : class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model, the model we build here is a simple fully connected deep neural network, containing 3 hidden layers and one output layer.\n",
    "#\n",
    "# Name the input layer for model metadata consistency.\n",
    "#\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=len(X.columns), name=\"dense\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "# We wrap the training with an mlflow wrapper to signify that this is an experiment run.\n",
    "# We also define a few more metrics at the very bottom to track the confusion matrix in MLFlow.\n",
    "\n",
    "epochs = 2\n",
    "history = model.fit(X_train, y_train, epochs=epochs, \\\n",
    "                    validation_data=(scaler.transform(X_val),y_val), \\\n",
    "                    verbose = True, class_weight = class_weights)\n",
    "\n",
    "y_pred_temp = model.predict(scaler.transform(X_test)) \n",
    "\n",
    "threshold = 0.995\n",
    "\n",
    "y_pred = np.where(y_pred_temp > threshold, 1,0)\n",
    "c_matrix = confusion_matrix(y_test,y_pred)\n",
    "ax = sns.heatmap(c_matrix, annot=True, cbar=False, cmap='Blues')\n",
    "ax.set_xlabel(\"Prediction\")\n",
    "ax.set_ylabel(\"Actual\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "t_n, f_p, f_n, t_p = c_matrix.ravel()\n",
    "print(f\"true -: {t_n}\")\n",
    "print(f\"false +: {f_p}\")\n",
    "print(f\"false -: {f_n}\")\n",
    "print(f\"true +: {t_p}\")\n",
    "\n",
    "model_proto,_ = tf2onnx.convert.from_keras(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model in onnx format and upload to S3 object storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Simple boto3 client function to upload a file to an S3 compatible object store.\n",
    "#\n",
    "\n",
    "import boto3\n",
    "import botocore.exceptions\n",
    "import os\n",
    "\n",
    "def upload_file(s3_client, file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param s3_client: The client connection handle. \n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used \n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "    try:    \n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "model_filename = 'fraud.onnx'\n",
    "onnx.save(model_proto, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Authenticate with the S3 service.\n",
    "#\n",
    "client = boto3.client(\n",
    "    's3',   \n",
    "    aws_access_key_id=os.getenv(\"FUSION_ACCESS_KEY\"),\n",
    "    aws_secret_access_key=os.getenv(\"FUSION_SECRET_KEY\"),\n",
    "    endpoint_url=os.getenv(\"FUSION_S3_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# print()\n",
    "# print(\"List buckets:\")\n",
    "# print(client.list_buckets())\n",
    "# print()\n",
    "\n",
    "print(\"Uploading file to S3...\") \n",
    "\n",
    "upload_file(client, model_filename, os.getenv(\"FUSION_VAULT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "1634c0bc43905e7916bfdb805d9fa90ddc101c0f948f75bff344e1199ec8d02f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
